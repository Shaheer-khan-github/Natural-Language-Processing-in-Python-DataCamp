{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practicing regular expressions: re.split() and re.findall()\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "The regular expression module re has already been imported for you.\n",
    "\n",
    "Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split my_string on each sentence ending. To do this:\n",
    "    * Write a pattern called sentence_endings to match sentence endings (.?!).\n",
    "\n",
    "    * Use re.split() to split my_string on the pattern and print the result.\n",
    "    \n",
    "* Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().\n",
    "    * Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n",
    "\n",
    "* Write a pattern called spaces to match one or more spaces (\"\\s+\") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.\n",
    "\n",
    "* Find all digits in my_string by writing a pattern called digits (\"\\d+\") and using re.findall(). Print the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "print(my_string)\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization with NLTK\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
    "* Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
    "* Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n",
    "* Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n",
    "* Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "print(scene_one)\n",
    "sentences = sent_tokenize(scene_one)\n",
    "print(sentences)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "print(tokenized_sent)\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = word_tokenize(scene_one)\n",
    "unique_tokens = set(unique_tokens)\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
