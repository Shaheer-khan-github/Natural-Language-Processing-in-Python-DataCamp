{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your first BOW\n",
    "A bag-of-words is an approach to transform text to numeric form.\n",
    "\n",
    "In this exercise, you will apply a BOW to the annak list before moving on to a larger dataset in the next exercise.\n",
    "\n",
    "Your task will be to work with this list and apply a BOW using the CountVectorizer(). This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment.\n",
    "\n",
    "Remember that the output of a CountVectorizer() is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the .toarray() method.\n",
    "\n",
    "Note that in this case you don't need to specify the max_features argument because the text is short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the count vectorizer function from sklearn.feature_extraction.text.\n",
    "* Build and fit the vectorizer on the small dataset.\n",
    "* Create the BOW representation with name anna_bow by calling the transform() method.\n",
    "* Print the BOW result as a dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW using product reviews\n",
    "You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called reviews. It contains two columns. The first one is called score and it is 0 when the review is negative, and 1 when it is positive. The second column is called review and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.\n",
    "\n",
    "Your task is to build a BOW vocabulary, using the review column.\n",
    "\n",
    "Remember that we can call the .get_feature_names() method on the vectorizer to obtain a list of all the vocabulary elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a CountVectorizer object, specifying the maximum number of features.\n",
    "* Fit the vectorizer.\n",
    "* Transform the fitted vectorizer.\n",
    "* Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify token sequence length with BOW\n",
    "We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.\n",
    "\n",
    "In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the review column and specify the sequence length of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build the vectorizer, specifying the token sequence length to be uni- and bigrams.\n",
    "* Fit the vectorizer.\n",
    "* Transform the fitted vectorizer.\n",
    "* In the DataFrame, make sure to correctly specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of vocabulary of movies reviews\n",
    "In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the movies reviews dataset. The first column is the review, which is of type object and the second column is the label, which is 0 for a negative review and 1 for a positive one.\n",
    "\n",
    "The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the movies dataset, limit the size of the vocabulary to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the movies dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the movies dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW with n-grams and vocabulary size\n",
    "In this exercise, you will practice building a bag-of-words once more, using the reviews dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the vectorizer from sklearn.\n",
    "* Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "* Fit the vectorizer to the review column.\n",
    "* Create a DataFrame from the BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize a string from GoT\n",
    "A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens).\n",
    "\n",
    "A string GoT has been created for you and it contains a quote from George R.R. Martin's Game of Thrones. Your task is to split it into individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the word tokenizing function from nltk.\n",
    "* Transform the GoT string to word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokens from the Avengers\n",
    "Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.\n",
    "\n",
    "A list avengers has been created for you. It contains a few quotes from the Avengers movies. You can explore it in the IPython Shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the required function and package.\n",
    "* Apply the word tokenizing function on each item of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A feature for the length of a review\n",
    "You have now worked with a string and a list with string items, it is time to use a larger sample of data.\n",
    "\n",
    "Your task in this exercise is to create a new feature for the length of a review, using the familiar reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the word tokenizing function from the required package.\n",
    "* Apply the function to the review column of the reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iterate over the created word_tokens list.\n",
    "* As you iterate, find the length of each item in the list and append it to the empty len_tokens list.\n",
    "* Create a new feature n_words in the reviews for the length of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the language of a string\n",
    "Sometimes you might need to analyze the sentiment of non-English text. Your first task in such a case will be to identify the foreign language.\n",
    "\n",
    "In this exercise you will identify the language of a single string. A string called foreign has been created for you. It has been randomly extracted from the reviews dataset and may contain some grammatical errors. Feel free to explore it in the IPython Shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the required function from the language detection package.\n",
    "* Detect the language of the foreign string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect language of a list of strings\n",
    "Now you will detect the language of each item in a list. A list called sentences has been created for you and it contains 3 sentences, each in a different language. They have been randomly extracted from the product reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iterate over the sentences in the list.\n",
    "* Detect the language of each sentence and append the detected language to the empty list languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in sentences:\n",
    "    languages.append(detect_langs(sentence))\n",
    "    \n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language detection of product reviews\n",
    "You will practice language detection on a small dataset called non_english_reviews. It is a sample of non-English reviews from the Amazon product reviews.\n",
    "\n",
    "You will iterate over the rows of the dataset, detecting the language of each row and appending it to an empty list. The list needs to be cleaned so that it only contains the language of the review such as 'en' for English instead of the regular output en:0.9987654. Remember that the language detection function might detect more than one language and the first item in the returned list is the most likely candidate. Finally, you will assign the list to a new column.\n",
    "\n",
    "The logic is the same as used in the slides and the exercise before but instead of applying the function to a list, you work with a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iterate over the rows of the non_english_reviews dataset.\n",
    "* Inside the loop, detect the language of the second column of the dataset.\n",
    "* Clean the string by splitting on a : inside the list comprehension expression.\n",
    "* Finally, assign the cleaned list to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "print(languages)\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "print(non_english_reviews.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92adc0dc7608e96a50ce5fb9acbd8bea0a01f16a513a55f8bb8294402fce5e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
